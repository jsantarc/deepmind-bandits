{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon-Greedy Bandit Algorithm\n",
    "\n",
    "## Installation\n",
    "First, install the required dependencies with pinned versions for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.24.3 matplotlib==3.7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The **Epsilon-Greedy** algorithm is one of the simplest and most widely used strategies for balancing exploration and exploitation in multi-armed bandit problems. It provides an elegant solution to the fundamental trade-off: should we exploit what we know works well, or explore to discover potentially better options?\n",
    "\n",
    "### The Exploration-Exploitation Dilemma\n",
    "\n",
    "In reinforcement learning, we face a constant dilemma:\n",
    "- **Exploitation**: Choose the action that currently appears best based on our estimates\n",
    "- **Exploration**: Try other actions to gather more information and potentially discover better options\n",
    "\n",
    "Pure exploitation can lead to suboptimal behavior if our initial estimates are wrong. Pure exploration wastes opportunities by not leveraging what we've learned. Epsilon-greedy provides a simple probabilistic solution.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "At each time step $t$, the algorithm:\n",
    "- With probability $\\epsilon$: **Explores** by choosing a random action (excluding the current best to ensure true exploration)\n",
    "- With probability $1-\\epsilon$: **Exploits** by choosing the action with the highest estimated value $Q_t(a)$\n",
    "\n",
    "This means that if $\\epsilon = 0.1$, the agent will explore 10% of the time and exploit 90% of the time.\n",
    "\n",
    "### Action Selection Rule\n",
    "\n",
    "The action selection at time $t$ follows:\n",
    "\n",
    "$$A_t = \\begin{cases}\n",
    "\\text{random action} \\neq \\arg\\max_a Q_t(a) & \\text{with probability } \\epsilon \\\\\n",
    "\\arg\\max_a Q_t(a) & \\text{with probability } 1-\\epsilon\n",
    "\\end{cases}$$\n",
    "\n",
    "Where $Q_t(a)$ represents our current estimate of the value of action $a$.\n",
    "\n",
    "### Value Update Rule\n",
    "\n",
    "The action-value estimates are updated using incremental sample averaging:\n",
    "\n",
    "$$Q_{t+1}(a) = Q_t(a) + \\frac{1}{N_t(a)}\\left[R_t - Q_t(a)\\right]$$\n",
    "\n",
    "This can be rewritten as:\n",
    "\n",
    "$$Q_{t+1}(a) = Q_t(a) + \\alpha_t(a) \\left[R_t - Q_t(a)\\right]$$\n",
    "\n",
    "where $\\alpha_t(a) = \\frac{1}{N_t(a)}$ is the step size.\n",
    "\n",
    "**Interpretation:**\n",
    "- $Q_t(a)$: Current estimate of action $a$'s value\n",
    "- $N_t(a)$: Number of times action $a$ has been selected (ensures we average over all samples)\n",
    "- $R_t$: Reward received from the most recent selection\n",
    "- $R_t - Q_t(a)$: Prediction error (how much we were wrong)\n",
    "\n",
    "### Algorithm Parameters\n",
    "\n",
    "- **`epsilon` ($\\epsilon$)**: Exploration rate, typically between 0.01 and 0.1\n",
    "  - Smaller values: More exploitation, faster convergence but risk of suboptimal solution\n",
    "  - Larger values: More exploration, slower convergence but better chance of finding optimal action\n",
    "  \n",
    "- **`initial_q`**: Initial Q-value estimates\n",
    "  - Setting high initial values encourages early exploration (\"optimistic initialization\")\n",
    "  - Setting to zero is neutral\n",
    "  \n",
    "- **`alfa` ($\\alpha$)**: Optional fixed step size\n",
    "  - If `None`: Uses sample average (decreasing step size $1/N_t(a)$)\n",
    "  - If fixed: Constant step size, gives more weight to recent rewards (useful for non-stationary problems)\n",
    "\n",
    "### Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- Simple to implement and understand\n",
    "- Computationally efficient\n",
    "- Guaranteed to explore all actions infinitely often\n",
    "- Works well in practice for many problems\n",
    "\n",
    "**Limitations:**\n",
    "- Explores uniformly at random (doesn't prioritize promising actions)\n",
    "- Fixed $\\epsilon$ doesn't adapt over time\n",
    "- May waste time on obviously bad actions\n",
    "- No uncertainty modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deepmind_bandits import EpsilonGreedyAlgorithm, GaussianBandits, BanditDataAnalyzer\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "We create a Gaussian bandit environment with 4 actions. Each action has a true mean reward (unknown to the agent) and some noise (standard deviation).\n",
    "\n",
    "**Environment Design:**\n",
    "- Action 0: Low mean (1.0), low noise - consistently mediocre\n",
    "- Action 1: High mean (2.0), moderate noise - **optimal action**\n",
    "- Action 2: Negative mean (-1.0), low noise - clearly bad\n",
    "- Action 3: Zero mean (0.0), high noise - unpredictable\n",
    "\n",
    "The agent must learn through interaction which action is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bandit environment with Gaussian reward distributions\n",
    "means = [1.0, 2.0, -1.0, 0.0]\n",
    "stds = [0.1, 0.2, 0.1, 0.3]\n",
    "env = GaussianBandits(means, stds)\n",
    "num_actions = env.num_arms\n",
    "\n",
    "print(f\"Number of actions: {num_actions}\")\n",
    "print(f\"True mean rewards: {means}\")\n",
    "print(f\"Reward noise (stds): {stds}\")\n",
    "print(f\"\\nOptimal action: {np.argmax(means)} with mean reward = {max(means):.2f}\")\n",
    "print(f\"\\nGoal: Learn to select action {np.argmax(means)} most often through trial and error!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Initialization\n",
    "\n",
    "We create an epsilon-greedy agent and a data analyzer to track its performance over time.\n",
    "\n",
    "**Agent Configuration:**\n",
    "- All Q-values initialized to 0 (neutral, no initial bias)\n",
    "- Exploration rate $\\epsilon = 0.1$ (10% random exploration)\n",
    "- Uses sample-average updates (step size = 1/N)\n",
    "\n",
    "**Analyzer:**\n",
    "- Tracks rewards, actions, and regret at each step\n",
    "- Computes cumulative metrics\n",
    "- Generates visualization plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create epsilon-greedy agent\n",
    "agent = EpsilonGreedyAlgorithm(num_actions=num_actions, initial_q=0.0)\n",
    "agent.epsilon = 0.1  # 10% exploration rate\n",
    "\n",
    "# Create analyzer for tracking performance metrics\n",
    "analyzer = BanditDataAnalyzer(means, num_actions)\n",
    "\n",
    "print(f\"Agent Configuration:\")\n",
    "print(f\"  Epsilon (exploration rate): {agent.epsilon}\")\n",
    "print(f\"  Initial Q-values: {agent.Q}\")\n",
    "print(f\"  Initial action counts: {agent.N}\")\n",
    "print(f\"\\nThe agent starts with no knowledge - all Q-values are 0!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now we run the main learning loop for 1000 time steps. At each step:\n",
    "\n",
    "1. **Select**: Agent chooses action using epsilon-greedy policy\n",
    "2. **Execute**: Action is taken in environment, reward is observed\n",
    "3. **Update**: Q-value estimate is updated based on observed reward\n",
    "4. **Track**: Performance metrics are recorded\n",
    "\n",
    "Over time, the agent should:\n",
    "- Build accurate estimates of each action's value\n",
    "- Increasingly select the optimal action (action 1)\n",
    "- Minimize cumulative regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000  # Number of time steps\n",
    "\n",
    "for t in range(T):\n",
    "    # Agent selects action using epsilon-greedy policy\n",
    "    action = agent.select_action_greedy()\n",
    "    \n",
    "    # Execute action and observe reward from environment\n",
    "    reward = env.pull_arm(action)\n",
    "    \n",
    "    # Update agent's Q-value estimates\n",
    "    agent.update_values(action, reward)\n",
    "    \n",
    "    # Track performance for analysis\n",
    "    analyzer.update_and_analyze(action, reward)\n",
    "\n",
    "print(f\"Training completed: {T} time steps\\n\")\n",
    "print(f\"Final Q-value estimates:\")\n",
    "for a in range(num_actions):\n",
    "    error = abs(agent.Q[a] - means[a])\n",
    "    print(f\"  Action {a}: Q = {agent.Q[a]:.3f} (true = {means[a]:.2f}, error = {error:.3f})\")\n",
    "    \n",
    "print(f\"\\nAction selection counts: {agent.N}\")\n",
    "print(f\"Total selections: {sum(agent.N)} (should equal {T})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Let's analyze how well the epsilon-greedy agent performed. We'll look at:\n",
    "- **Q-value convergence**: Did estimates converge to true values?\n",
    "- **Action selection**: Did we find and exploit the optimal action?\n",
    "- **Regret**: How much reward did we lose by not always picking optimally?\n",
    "- **Cumulative reward**: Total reward accumulated over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize analysis (compute cumulative metrics)\n",
    "analyzer.finalize_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Value Progression Over Time\n",
    "\n",
    "This plot shows how the agent's Q-value estimates evolved during learning:\n",
    "- **Solid colored lines**: Q-values for each action over time\n",
    "- **Dashed lines**: True mean rewards (ground truth)\n",
    "- **Red arrows**: When the agent switches between actions\n",
    "\n",
    "**What to look for:**\n",
    "- Q-values should converge toward the true means (dashed lines)\n",
    "- More frequently selected actions have smoother estimates\n",
    "- Early exploration causes more switching (red arrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_Qvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Regret\n",
    "\n",
    "**Regret** measures the difference between the reward we *could* have gotten (always picking optimal) and what we *actually* got.\n",
    "\n",
    "At each step $t$:\n",
    "- Instantaneous regret = $r^* - r_t$ where $r^*$ is the optimal mean reward\n",
    "- Cumulative regret = sum of all instantaneous regrets up to time $t$\n",
    "\n",
    "**What to look for:**\n",
    "- **Slope**: Should decrease over time as we learn\n",
    "- **Final value**: Lower is better (less total regret)\n",
    "- **Shape**: Linear growth means we're not learning; sublinear (flattening) means we're improving\n",
    "\n",
    "For epsilon-greedy, regret grows *linearly* in the long run because we always explore with probability $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_regret()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Reward\n",
    "\n",
    "This shows the total reward accumulated over time:\n",
    "- **Black line**: Total cumulative reward across all actions\n",
    "- **Dashed lines**: Cumulative reward per action\n",
    "\n",
    "**What to look for:**\n",
    "- Steeper slope = higher reward rate\n",
    "- The optimal action's line should dominate\n",
    "- Total reward should increase steadily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_cumulative_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary and Analysis\n",
    "\n",
    "Let's quantify how well the agent learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EPSILON-GREEDY PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nExperiment Parameters:\")\n",
    "print(f\"  Total time steps: {T}\")\n",
    "print(f\"  Exploration rate (epsilon): {agent.epsilon}\")\n",
    "print(f\"  Expected exploration steps: ~{int(T * agent.epsilon)} ({agent.epsilon * 100:.0f}%)\")\n",
    "\n",
    "print(f\"\\nLearned Q-Values vs True Means:\")\n",
    "for a in range(num_actions):\n",
    "    q_val = agent.Q[a]\n",
    "    true_mean = means[a]\n",
    "    error = abs(q_val - true_mean)\n",
    "    count = agent.N[a]\n",
    "    pct = 100 * count / T\n",
    "    marker = \" ← OPTIMAL\" if a == np.argmax(means) else \"\"\n",
    "    print(f\"  Action {a}: Q={q_val:6.3f}, True={true_mean:5.2f}, Error={error:.3f} | \"\n",
    "          f\"Selected {count:4d} times ({pct:5.1f}%){marker}\")\n",
    "\n",
    "optimal_action = np.argmax(means)\n",
    "optimal_selections = agent.N[optimal_action]\n",
    "optimal_pct = 100 * optimal_selections / T\n",
    "\n",
    "print(f\"\\nOptimal Action Performance:\")\n",
    "print(f\"  Optimal action: {optimal_action} (true mean = {means[optimal_action]:.2f})\")\n",
    "print(f\"  Times selected: {optimal_selections}/{T} ({optimal_pct:.1f}%)\")\n",
    "print(f\"  Theoretical maximum (1-ε): {100*(1-agent.epsilon):.1f}%\")\n",
    "\n",
    "if optimal_pct >= 100 * (1 - agent.epsilon) - 5:  # within 5% of theoretical max\n",
    "    print(f\"  ✓ Successfully learned to exploit optimal action!\")\n",
    "else:\n",
    "    print(f\"  ⚠ Could improve - not yet converged to optimal policy\")\n",
    "\n",
    "print(f\"\\nRegret Analysis:\")\n",
    "print(f\"  Final cumulative regret: {analyzer.regret[-1]:.2f}\")\n",
    "print(f\"  Average regret per step: {analyzer.regret[-1]/T:.3f}\")\n",
    "expected_regret_rate = agent.epsilon * (means[optimal_action] - np.mean(means))\n",
    "print(f\"  Theoretical regret rate: ~{expected_regret_rate:.3f} per step\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**What we learned:**\n",
    "1. ✅ Epsilon-greedy successfully balances exploration and exploitation\n",
    "2. ✅ Q-value estimates converge to true means with enough samples  \n",
    "3. ✅ The optimal action is identified and selected most often\n",
    "4. ⚠️ Continuous exploration (fixed $\\epsilon$) causes linear regret growth\n",
    "\n",
    "**Improvements to consider:**\n",
    "- **Decaying epsilon**: Reduce $\\epsilon$ over time (e.g., $\\epsilon_t = \\epsilon_0/t$)\n",
    "- **Optimistic initialization**: Start with high Q-values to encourage early exploration\n",
    "- **UCB**: Use uncertainty-based exploration instead of random\n",
    "- **Thompson Sampling**: Bayesian approach that naturally balances exploration/exploitation\n",
    "\n",
    "**When to use epsilon-greedy:**\n",
    "- Simple baseline for bandit problems\n",
    "- When computational efficiency matters\n",
    "- When you want interpretable, predictable behavior\n",
    "- As a starting point before trying more sophisticated methods"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Exercises\n\n### Exercise 1: Experiment with Different Reward Distributions\n\nTry modifying the bandit environment with different reward distributions. How does the epsilon-greedy algorithm perform when:\n- The optimal action has high variance?\n- Multiple actions have similar mean rewards?\n- The worst action has very high variance?\n\nModify the `means` and `stds` arrays and re-run the training loop to observe the behavior.\n\n<details>\n    <summary>Click here for hint</summary>\n\n```python\n# Example 1: High variance optimal action\nmeans_high_var = [1.0, 2.5, 1.2, 0.5]\nstds_high_var = [0.1, 1.5, 0.1, 0.1]  # Optimal action has high noise!\n\n# Example 2: Similar mean rewards (harder to distinguish)\nmeans_similar = [1.0, 1.2, 1.1, 0.9]\nstds_similar = [0.1, 0.1, 0.1, 0.1]\n\n# Example 3: High variance bad action\nmeans_trap = [1.5, 2.0, 0.5, -1.0]\nstds_trap = [0.1, 0.1, 0.1, 3.0]  # Bad action has huge variance\n\n# Create new environment\nenv_new = GaussianBandits(means_high_var, stds_high_var)\n\n# Reset and retrain agent\nagent_new = EpsilonGreedyAlgorithm(num_actions=len(means_high_var), initial_q=0.0)\nagent_new.epsilon = 0.1\nanalyzer_new = BanditDataAnalyzer(means_high_var, len(means_high_var))\n\n# Run training loop\nfor t in range(1000):\n    action = agent_new.select_action_greedy()\n    reward = env_new.pull_arm(action)\n    agent_new.update_values(action, reward)\n    analyzer_new.update_and_analyze(action, reward)\n\n# Analyze results\nanalyzer_new.finalize_analysis()\nanalyzer_new.plot_Qvalue()\nanalyzer_new.plot_regret()\n```\n\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Exercise 2: Compare Different Epsilon Values\n\nThe exploration rate $\\epsilon$ controls the exploration-exploitation trade-off. Experiment with different values:\n- Low epsilon (e.g., 0.01): More exploitation\n- Medium epsilon (e.g., 0.1): Balanced approach\n- High epsilon (e.g., 0.3): More exploration\n\nCompare their performance in terms of regret, convergence speed, and optimal action selection rate.\n\n<details>\n    <summary>Click here for hint</summary>\n\n```python\n# Test different epsilon values\nepsilon_values = [0.01, 0.05, 0.1, 0.2, 0.3]\nresults = {}\n\n# Use the original environment\nenv_test = GaussianBandits(means, stds)\n\nfor eps in epsilon_values:\n    # Create fresh agent with this epsilon\n    agent_test = EpsilonGreedyAlgorithm(num_actions=num_actions, initial_q=0.0)\n    agent_test.epsilon = eps\n    analyzer_test = BanditDataAnalyzer(means, num_actions)\n    \n    # Train\n    for t in range(1000):\n        action = agent_test.select_action_greedy()\n        reward = env_test.pull_arm(action)\n        agent_test.update_values(action, reward)\n        analyzer_test.update_and_analyze(action, reward)\n    \n    analyzer_test.finalize_analysis()\n    \n    # Store results\n    results[eps] = {\n        'regret': analyzer_test.regret[-1],\n        'optimal_pct': 100 * agent_test.N[np.argmax(means)] / 1000,\n        'Q_values': agent_test.Q.copy()\n    }\n    \n    print(f\"ε={eps:.2f}: Regret={results[eps]['regret']:.2f}, \"\n          f\"Optimal%={results[eps]['optimal_pct']:.1f}%\")\n\n# Plot comparison\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.bar([str(e) for e in epsilon_values], [results[e]['regret'] for e in epsilon_values])\nplt.xlabel('Epsilon')\nplt.ylabel('Cumulative Regret')\nplt.title('Regret vs Epsilon')\n\nplt.subplot(1, 2, 2)\nplt.bar([str(e) for e in epsilon_values], [results[e]['optimal_pct'] for e in epsilon_values])\nplt.axhline(y=100, color='r', linestyle='--', label='Perfect (100%)')\nplt.xlabel('Epsilon')\nplt.ylabel('Optimal Action %')\nplt.title('Optimal Action Selection vs Epsilon')\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n</details>",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}