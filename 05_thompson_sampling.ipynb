{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Thompson Sampling for Gaussian Bandits\n",
    "\n",
    "## Installation\n",
    "First, install the required dependencies with pinned versions for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.24.3 matplotlib==3.7.2 scipy==1.11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## Overview\n\n**Thompson Sampling** is a smart way to solve the multi-armed bandit problem. Instead of randomly exploring (like epsilon-greedy) or using confidence bounds (like UCB), it uses a more natural approach: it tries actions based on how likely they are to be the best.\n\n### The Core Idea\n\nThe basic idea is simple:\n- For each action, keep track of what we know and **how certain** we are\n- Actions we're **uncertain** about → might be good, so try them occasionally\n- Actions we're **confident** are good → try them often\n- Actions we're **confident** are bad → avoid them\n\nThink of it like this: if you're unsure whether a restaurant is good, you might give it a chance. But if you've tried it many times and know it's bad, you'll avoid it.\n\n### How It Works\n\nIn Thompson Sampling, we select the action $A_t$ at time $t$ as:\n\n$$A_t = \\arg\\max_a \\tilde Q_t(a),$$\n\nwhere $\\tilde Q_t(a)$ is a **random sample** from what we believe about action $a$.\n\nFor Gaussian rewards, we sample from:\n\n$$\\tilde Q_t(a) \\sim \\mathcal N\\!\\left(q(a)|\\mu_{a,t},\\,\\tau_{a,t}^2\\right).$$\n\n**What this means:**\n- $\\mu_{a,t}$: Our best guess of action $a$'s true reward\n- $\\tau_{a,t}^2$: How uncertain we are (big number = very uncertain)\n- As we try action $a$ more, uncertainty shrinks\n\n### Why This Works\n\nThompson Sampling naturally balances exploration and exploitation:\n\n1. **Early on (high uncertainty)**:\n   - We're unsure about all actions\n   - Each action occasionally looks best → gets tried\n\n2. **Over time (learning)**:\n   - Good actions: Get higher estimates, become more certain\n   - Bad actions: Get lower estimates, become more certain\n   - Good actions with low uncertainty → selected most often\n\n3. **Eventually**:\n   - We're confident about which action is best\n   - Mostly select the optimal action\n   - Still occasionally try others (but rarely)\n\n### Advantages\n\n**vs. Epsilon-Greedy:**\n- ✅ No need to pick an epsilon value\n- ✅ Exploration naturally decreases over time\n- ✅ Much better performance\n\n**vs. UCB:**\n- ✅ Often works better in practice\n- ✅ More flexible for complex problems\n\n### Main Idea\n\nThompson Sampling achieves great performance with no parameters to tune!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deepmind_bandits import ThompsonSampling, GaussianBandits, BanditDataAnalyzer\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Illustration: Sampling from Posteriors\n",
    "\n",
    "To build intuition, let's visualize how Thompson Sampling uses uncertainty to drive exploration.\n",
    "\n",
    "Consider three actions with Gaussian posterior distributions:\n",
    "- **Action C (Green)**: Highest mean ($\\mu=2.0$), low variance ($\\sigma=0.3$)\n",
    "- **Action B (Orange)**: Medium mean ($\\mu=1.5$), **high variance** ($\\sigma=1.0$)\n",
    "- **Action A (Blue)**: Lowest mean ($\\mu=0.0$), low variance ($\\sigma=0.3$)\n",
    "\n",
    "**Question**: Which action should we choose?\n",
    "\n",
    "**Greedy answer**: Always choose C (highest mean)\n",
    "\n",
    "**Thompson Sampling's probabilistic answer**:\n",
    "- C has the highest mean, so it will be sampled most often\n",
    "- But B has high uncertainty — occasionally it samples values *higher* than C\n",
    "- When B samples high, we select it and learn more about it\n",
    "- As we observe B's rewards, its variance shrinks and we become certain whether it's better or worse than C\n",
    "\n",
    "This is **optimism under uncertainty** in probabilistic form: we give uncertain actions a chance proportional to the probability they could be best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three Gaussian distributions\n",
    "q_ = [0.0, 1.5, 2.0]     # different means\n",
    "stds = [0.3, 1.0, 0.3]   # different standard deviations\n",
    "labels = [\"A\", \"B\", \"C\"]\n",
    "x = np.linspace(-1, 4, 400)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for mu, sigma, label in zip(q_, stds, labels):\n",
    "    y = 1/(sigma*np.sqrt(2*np.pi)) * np.exp(-0.5*((x-mu)/sigma)**2)\n",
    "    plt.plot(x, y, label=f'{label}: μ={mu}, σ={sigma}', linewidth=2)\n",
    "\n",
    "plt.title(\"Three Gaussian Posteriors\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Reward Value\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "## Bayesian Inference for Gaussian Rewards\n\nThompson Sampling needs to track what we know about each action. For Gaussian rewards, we use **Gaussian distributions** to represent our beliefs.\n\n### The Math (Simplified)\n\nAfter trying action $a$ multiple times ($n_a$ times) with sample mean $\\hat{\\mu}_{a,t}$, we update our belief:\n\n**Uncertainty (variance):**\n$$\\tau_{a,t}^2 = \\frac{\\sigma^2}{n_a}$$\n\n**Best estimate (mean):**\n$$\\mu_{a,t}=\\hat\\mu_{a,t}$$\n\n**What this means:**\n- More tries ($n_a$ bigger) → less uncertainty ($\\tau_{a,t}^2$ smaller)\n- Our best guess is just the average reward we've seen\n- Uncertainty shrinks as we collect more data\n\n### Estimating Noise\n\nWe don't know the reward noise $\\sigma^2$ ahead of time, so we estimate it from all the rewards we've seen:\n\n$$\\hat\\sigma^2 = \\frac{1}{\\,N-1\\,}\\sum_{i=1}^{t-1}(r_i-\\bar r)^2$$\n\nThis is just the sample variance across all observations.\n\n### Implementation Note\n\nOur `ThompsonSampling` class:\n- Uses a flat prior (no initial bias)\n- Updates beliefs after each reward\n- Estimates noise variance online"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "We create the same 4-action Gaussian bandit environment used in previous notebooks:\n",
    "- Action 0: Low mean (1.0), low noise\n",
    "- Action 1: **High mean (2.0), moderate noise** — optimal action\n",
    "- Action 2: Negative mean (-1.0), low noise — clearly bad\n",
    "- Action 3: Zero mean (0.0), high noise — unpredictable\n",
    "\n",
    "This environment lets us compare Thompson Sampling's performance against epsilon-greedy and UCB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bandit environment\n",
    "means = [1.0, 2.0, -1.0, 0.0]\n",
    "stds = [0.1, 0.2, 0.1, 0.3]\n",
    "env = GaussianBandits(means, stds)\n",
    "num_actions = env.num_arms\n",
    "\n",
    "print(f\"Number of actions: {num_actions}\")\n",
    "print(f\"True mean rewards: {means}\")\n",
    "print(f\"Reward noise (stds): {stds}\")\n",
    "print(f\"\\nOptimal action: {np.argmax(means)} with mean reward = {max(means):.2f}\")\n",
    "print(f\"\\nGoal: Learn the optimal action through Bayesian reasoning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Agent Initialization\n",
    "\n",
    "We create a Thompson Sampling agent with:\n",
    "- **Flat prior** ($\\tau_0^2 = \\infty$): No initial bias, let data drive learning\n",
    "- **Initial Q-values**: 0.0 (neutral starting point)\n",
    "- **Prior mean** $\\mu_0 = 0.0$: Center of flat prior (doesn't matter with $\\tau_0^2=\\infty$)\n",
    "\n",
    "The agent will:\n",
    "1. Maintain Gaussian posterior $\\mathcal{N}(\\mu_{a,t}, \\tau_{a,t}^2)$ for each action\n",
    "2. At each step, sample from all posteriors and select action with highest sample\n",
    "3. Update posteriors based on observed rewards\n",
    "4. Naturally explore uncertain actions and exploit confident good actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Thompson Sampling agent with flat prior\n",
    "agent = ThompsonSampling(num_actions=num_actions, initial_q=0.0, tau0_var=np.inf, mu0=0.0)\n",
    "\n",
    "# Create analyzer for tracking performance\n",
    "analyzer = BanditDataAnalyzer(means, num_actions)\n",
    "\n",
    "print(f\"Agent Configuration:\")\n",
    "print(f\"  Prior: Flat (tau0_var = inf) — no initial bias\")\n",
    "print(f\"  Initial posterior means: {agent.means}\")\n",
    "print(f\"  Initial posterior variances: {agent.tau}\")\n",
    "print(f\"\\nThe agent starts with maximum uncertainty about all actions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now we run the Thompson Sampling algorithm for 1000 time steps. At each step:\n",
    "\n",
    "1. **Sample from posteriors**: Draw $\\tilde{Q}_t(a) \\sim \\mathcal{N}(\\mu_{a,t}, \\tau_{a,t}^2)$ for each action $a$\n",
    "2. **Select action**: Choose $A_t = \\arg\\max_a \\tilde{Q}_t(a)$ (action with highest sampled value)\n",
    "3. **Observe reward**: Execute action in environment, receive reward $R_t \\sim \\mathcal{N}(q(A_t), \\sigma^2)$\n",
    "4. **Update posteriors**: \n",
    "   - Update pooled variance estimate $\\hat{\\sigma}^2$ with new reward\n",
    "   - Update sample mean $\\hat{\\mu}_{A_t,t}$ for selected action\n",
    "   - Recompute posterior parameters $\\mu_{A_t,t+1}$ and $\\tau_{A_t,t+1}^2$\n",
    "5. **Track metrics**: Record for analysis\n",
    "\n",
    "Over time, we expect:\n",
    "- Posterior means $\\mu_{a,t}$ to converge to true means $q(a)$\n",
    "- Posterior variances $\\tau_{a,t}^2$ to shrink toward zero (for frequently selected actions)\n",
    "- Optimal action to be selected with increasing frequency\n",
    "- Suboptimal regret growth to be logarithmic $O(\\log T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000  # Number of time steps\n",
    "\n",
    "for t in range(T):\n",
    "    # Agent samples from posteriors and selects action\n",
    "    action = agent.select_action()\n",
    "    \n",
    "    # Execute action in environment and observe reward\n",
    "    reward = env.pull_arm(action)\n",
    "    \n",
    "    # Update posterior distributions (Bayesian inference)\n",
    "    agent.update_values(action, reward)\n",
    "    \n",
    "    # Track performance\n",
    "    analyzer.update_and_analyze(action, reward)\n",
    "\n",
    "print(f\"Training completed: {T} time steps\\n\")\n",
    "print(f\"Final Results:\")\n",
    "print(f\"  Posterior means: {agent.means}\")\n",
    "print(f\"  Posterior variances: {agent.tau}\")\n",
    "print(f\"  Action selection counts: {agent.N}\")\n",
    "print(f\"  Pooled variance estimate: {agent.pooled_var:.4f}\")\n",
    "print(f\"\\nCompare posterior means to true means: {means}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Let's analyze Thompson Sampling's performance and compare it to epsilon-greedy and UCB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize analysis\n",
    "analyzer.finalize_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Q-Value Progression Over Time\n",
    "\n",
    "This plot shows how posterior means (Q-values) evolved during learning:\n",
    "- **Solid colored lines**: Posterior mean $\\mu_{a,t}$ for each action\n",
    "- **Dashed lines**: True mean rewards (ground truth)\n",
    "- **Red arrows**: Action switches\n",
    "\n",
    "**What to look for:**\n",
    "- Posterior means should converge to true means (solid → dashed)\n",
    "- Frequently selected actions converge faster (more data)\n",
    "- Bad actions (action 2) may not converge if rarely selected (efficient!)\n",
    "- Early exploration → many switches → rapid learning\n",
    "- Late exploitation → few switches → stable optimal behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_Qvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Cumulative Regret\n",
    "\n",
    "**Regret** measures the total reward lost by not always selecting the optimal action:\n",
    "\n",
    "$$\\text{Regret}_T = \\sum_{t=1}^{T} \\left(q^* - q(A_t)\\right)$$\n",
    "\n",
    "where $q^* = \\max_a q(a)$ is the optimal mean reward.\n",
    "\n",
    "**What to look for:**\n",
    "- **Slope**: Should flatten over time (sublinear growth)\n",
    "- **Shape**: Logarithmic $O(\\log T)$ → curve flattens significantly\n",
    "- **Comparison**: \n",
    "  - Epsilon-greedy: Linear growth $O(T)$ (never stops exploring)\n",
    "  - UCB: Logarithmic $O(\\log T)$ (similar theoretical bound)\n",
    "  - Thompson Sampling: Logarithmic $O(\\log T)$ with better constants in practice\n",
    "\n",
    "Thompson Sampling typically achieves lower regret than UCB in finite horizons due to better constant factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_regret()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Cumulative Reward\n",
    "\n",
    "This shows total accumulated reward over time:\n",
    "- **Black line**: Total cumulative reward across all actions\n",
    "- **Dashed lines**: Cumulative reward per action\n",
    "\n",
    "**What to look for:**\n",
    "- Steeper slope = higher reward rate (better performance)\n",
    "- Action 1's line (optimal) should dominate and have steepest slope\n",
    "- Total reward should grow steadily, accelerating as we converge to optimal action\n",
    "- Compare slope to theoretical maximum: $q^* \\times T = 2.0 \\times 1000 = 2000$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_cumulative_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"THOMPSON SAMPLING PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nExperiment Parameters:\")\n",
    "print(f\"  Total time steps: {T}\")\n",
    "print(f\"  Prior: Flat (uninformative)\")\n",
    "print(f\"  Pooled variance estimate: {agent.pooled_var:.4f}\")\n",
    "\n",
    "print(f\"\\nLearned Posterior Distributions:\")\n",
    "for a in range(num_actions):\n",
    "    mu = agent.means[a]\n",
    "    tau_sq = agent.tau[a]\n",
    "    true_mean = means[a]\n",
    "    error = abs(mu - true_mean)\n",
    "    count = agent.N[a]\n",
    "    pct = 100 * count / T\n",
    "    marker = \" ← OPTIMAL\" if a == np.argmax(means) else \"\"\n",
    "    print(f\"  Action {a}: μ={mu:6.3f}, τ²={tau_sq:.4f} (true={true_mean:5.2f}, error={error:.3f}) | \"\n",
    "          f\"Selected {count:4d} times ({pct:5.1f}%){marker}\")\n",
    "\n",
    "optimal_action = np.argmax(means)\n",
    "optimal_selections = agent.N[optimal_action]\n",
    "optimal_pct = 100 * optimal_selections / T\n",
    "\n",
    "print(f\"\\nOptimal Action Performance:\")\n",
    "print(f\"  Optimal action: {optimal_action} (true mean = {means[optimal_action]:.2f})\")\n",
    "print(f\"  Times selected: {optimal_selections}/{T} ({optimal_pct:.1f}%)\")\n",
    "\n",
    "if optimal_pct >= 85:  # Thompson Sampling should achieve >85% for optimal action\n",
    "    print(f\"  ✓ Excellent! Converged to optimal action with high probability.\")\n",
    "elif optimal_pct >= 70:\n",
    "    print(f\"  ✓ Good convergence to optimal action.\")\n",
    "else:\n",
    "    print(f\"  ⚠ Could improve - may need more time steps or different initialization.\")\n",
    "\n",
    "print(f\"\\nRegret Analysis:\")\n",
    "print(f\"  Final cumulative regret: {analyzer.regret[-1]:.2f}\")\n",
    "print(f\"  Average regret per step: {analyzer.regret[-1]/T:.3f}\")\n",
    "print(f\"  Expected behavior: Logarithmic growth O(log T)\")\n",
    "\n",
    "print(f\"\\nPosterior Uncertainty:\")\n",
    "for a in range(num_actions):\n",
    "    sigma = np.sqrt(agent.tau[a])\n",
    "    print(f\"  Action {a}: σ = {sigma:.4f} (std dev of posterior)\")\n",
    "print(f\"\\nLower variance = higher confidence. Rarely-selected actions retain high variance.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Visualize Posterior Distributions\n",
    "\n",
    "This plot shows the **final posterior belief** about each action's reward:\n",
    "- **Solid curves**: Posterior distributions $\\mathcal{N}(\\mu_{a,T}, \\tau_{a,T}^2)$\n",
    "- **Dashed vertical lines**: True mean rewards\n",
    "\n",
    "**What to look for:**\n",
    "- **Width** (variance): Narrow = high confidence, Wide = high uncertainty\n",
    "- **Center** (mean): Should align with dashed line (true mean)\n",
    "- **Optimal action** (action 1): Should have narrow distribution centered at 2.0\n",
    "- **Rarely-selected actions**: May have wider distributions (less data)\n",
    "- **Bad actions**: Should have narrow distributions centered at low values (confidently bad)\n",
    "\n",
    "This visualization shows how Thompson Sampling's belief state drives its behavior: \n",
    "- When distributions overlap significantly → exploration still needed\n",
    "- When optimal action's distribution is clearly highest and narrow → confident exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final posterior distributions\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Determine plot range\n",
    "all_means = means + list(agent.means)\n",
    "x_min = min(all_means) - 2\n",
    "x_max = max(all_means) + 2\n",
    "x = np.linspace(x_min, x_max, 500)\n",
    "\n",
    "for a in range(num_actions):\n",
    "    mu = agent.means[a]\n",
    "    sigma = np.sqrt(agent.tau[a])\n",
    "    \n",
    "    # Gaussian PDF\n",
    "    y = 1/(sigma*np.sqrt(2*np.pi)) * np.exp(-0.5*((x-mu)/sigma)**2)\n",
    "    label_suffix = \" (OPTIMAL)\" if a == np.argmax(means) else \"\"\n",
    "    plt.plot(x, y, label=f'Action {a}: μ={mu:.2f}, σ={sigma:.2f}{label_suffix}', linewidth=2)\n",
    "    \n",
    "    # Mark true mean\n",
    "    plt.axvline(x=means[a], color=f'C{a}', linestyle='--', alpha=0.3, linewidth=1.5)\n",
    "\n",
    "plt.title(\"Final Posterior Distributions (Thompson Sampling)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Reward Value\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Dashed vertical lines show true means for comparison.\")\n",
    "print(\"Narrow distributions = high confidence, Wide distributions = high uncertainty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Sampling Behavior Demonstration\n",
    "\n",
    "To understand how Thompson Sampling selects actions, let's simulate 10 decision steps using the final learned posteriors:\n",
    "\n",
    "At each trial:\n",
    "1. Sample once from each action's posterior: $\\tilde{Q}(a) \\sim \\mathcal{N}(\\mu_a, \\tau_a^2)$\n",
    "2. Select action with highest sample: $A = \\arg\\max_a \\tilde{Q}(a)$\n",
    "\n",
    "**What to expect:**\n",
    "- Optimal action (action 1) should be selected most often (high mean, low variance)\n",
    "- Occasionally other actions may be selected due to random sampling\n",
    "- High-variance actions have more variable samples → occasional exploration\n",
    "- This probabilistic behavior is the essence of Thompson Sampling\n",
    "\n",
    "**Note**: This uses the *final* learned posteriors. Early in training, all actions would be selected more evenly due to high uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how sampling works\n",
    "print(\"=\" * 70)\n",
    "print(\"SAMPLING FROM FINAL POSTERIORS (10 TRIALS)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nThis demonstrates Thompson Sampling's probabilistic action selection:\\n\")\n",
    "\n",
    "selection_counts = np.zeros(num_actions, dtype=int)\n",
    "\n",
    "for trial in range(10):\n",
    "    samples = []\n",
    "    for a in range(num_actions):\n",
    "        sample = np.random.normal(agent.means[a], np.sqrt(agent.tau[a]))\n",
    "        samples.append(sample)\n",
    "    \n",
    "    selected = np.argmax(samples)\n",
    "    selection_counts[selected] += 1\n",
    "    \n",
    "    samples_str = [f'{s:5.2f}' for s in samples]\n",
    "    marker = \" ← OPTIMAL\" if selected == np.argmax(means) else \"\"\n",
    "    print(f\"Trial {trial+1:2d}: Samples = [{', '.join(samples_str)}], Selected = {selected}{marker}\")\n",
    "\n",
    "print(f\"\\nSelection frequency: {selection_counts}\")\n",
    "print(f\"True optimal action: {np.argmax(means)}\")\n",
    "print(f\"\\nOptimal action selected {selection_counts[np.argmax(means)]}/10 times in this demo.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. ✅ **Bayesian Exploration Works**: Thompson Sampling naturally balances exploration and exploitation through probabilistic reasoning\n",
    "2. ✅ **Efficient Learning**: Achieves logarithmic regret $O(\\log T)$ like UCB, often with better constants\n",
    "3. ✅ **Adaptive Exploration**: Exploration decreases automatically as uncertainty decreases (no need to tune $\\epsilon$)\n",
    "4. ✅ **Optimal Convergence**: Posterior distributions converge to true reward distributions\n",
    "5. ✅ **Probability Matching**: Selects each action with probability proportional to it being optimal\n",
    "\n",
    "### Advantages of Thompson Sampling\n",
    "\n",
    "**vs. Epsilon-Greedy:**\n",
    "- ✅ No parameter tuning ($\\epsilon$ is hard to set correctly)\n",
    "- ✅ Logarithmic vs linear regret ($O(\\log T)$ vs $O(T)$)\n",
    "- ✅ Exploration driven by uncertainty, not randomness\n",
    "- ✅ Naturally decreasing exploration over time\n",
    "\n",
    "**vs. UCB:**\n",
    "- ✅ Better empirical performance (lower constant factors in regret bound)\n",
    "- ✅ More natural extension to complex problems (contextual bandits, RL)\n",
    "- ✅ Handles non-stationary environments better (can use dynamic priors)\n",
    "- ✅ Principled Bayesian framework\n",
    "\n",
    "### When to Use Thompson Sampling\n",
    "\n",
    "**Ideal scenarios:**\n",
    "- ✅ You can model reward distributions (Gaussian, Bernoulli, etc.)\n",
    "- ✅ You want state-of-the-art empirical performance\n",
    "- ✅ Problem may extend to contextual bandits or RL (Thompson Sampling generalizes well)\n",
    "- ✅ You prefer a principled Bayesian approach\n",
    "- ✅ You want automatic exploration-exploitation balance\n",
    "\n",
    "**Avoid when:**\n",
    "- ⚠️ Reward distribution is unknown and can't be approximated\n",
    "- ⚠️ You need provable worst-case guarantees (UCB has tighter theory)\n",
    "- ⚠️ Extreme computational constraints (sampling can be slower than UCB's deterministic selection)\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "1. **Start with flat priors** ($\\tau_0^2 = \\infty$) unless you have genuine prior knowledge\n",
    "2. **For Bernoulli rewards** (e.g., click-through rates), use Beta-Bernoulli Thompson Sampling\n",
    "3. **For unknown noise variance**, use pooled variance estimation as shown here\n",
    "4. **For non-stationary problems**, use discounted posteriors or sliding windows\n",
    "5. **For production systems**, Thompson Sampling is industry standard (widely used in A/B testing, recommendation systems, ad placement)\n",
    "\n",
    "### Comparison Summary\n",
    "\n",
    "| Algorithm | Regret | Exploration | Tuning | Empirical Performance |\n",
    "|-----------|--------|-------------|--------|----------------------|\n",
    "| Epsilon-Greedy | $O(T)$ | Random | Hard ($\\epsilon$) | Poor |\n",
    "| UCB | $O(\\log T)$ | Deterministic | Easy ($c$) | Good |\n",
    "| **Thompson Sampling** | $O(\\log T)$ | Probabilistic | **None** | **Excellent** |\n",
    "\n",
    "**Bottom line**: Thompson Sampling is the gold standard for bandit problems where you can model the reward distribution. It combines strong theoretical guarantees with excellent empirical performance and no parameter tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}