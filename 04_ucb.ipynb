{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper Confidence Bound (UCB) Algorithm\n",
    "\n",
    "## Installation\n",
    "First, install the required dependencies with pinned versions for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.24.3 matplotlib==3.7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The **Upper Confidence Bound (UCB)** algorithm is a sophisticated approach to the exploration-exploitation trade-off that uses **optimism in the face of uncertainty**. Unlike epsilon-greedy which explores randomly, UCB systematically explores actions based on their uncertainty.\n",
    "\n",
    "### The Core Principle: Optimism Under Uncertainty\n",
    "\n",
    "The key insight of UCB is simple but powerful:\n",
    "> \"Actions that have been tried less often are more uncertain. We should be optimistic about uncertain actions and give them a chance.\"\n",
    "\n",
    "Instead of randomly exploring, UCB adds an **uncertainty bonus** to each action's estimated value. Actions with:\n",
    "- **High estimated value**: Already look good\n",
    "- **High uncertainty** (tried rarely): Get a bonus because we don't know much about them\n",
    "\n",
    "### UCB Action-Selection Rule\n",
    "\n",
    "At each time step $t$, UCB selects the action that maximizes:\n",
    "\n",
    "$$A_t = \\arg\\max_a \\left( Q_t(a) + U_t(a) \\right)$$\n",
    "\n",
    "Where:\n",
    "- $Q_t(a)$ is the **estimated value** of action $a$ (exploitation term)\n",
    "- $U_t(a)$ is the **uncertainty bonus** (exploration term)\n",
    "\n",
    "This is called the **Upper Confidence Bound** because $Q_t(a) + U_t(a)$ represents an optimistic upper bound on the true value of action $a$.\n",
    "\n",
    "### The Uncertainty Bonus\n",
    "\n",
    "The uncertainty term is defined as:\n",
    "\n",
    "$$U_t(a) = c\\sqrt{\\frac{\\log t}{N_t(a)}}$$\n",
    "\n",
    "**Breaking this down:**\n",
    "- $N_t(a)$: Number of times action $a$ has been selected\n",
    "  - In denominator → Uncertainty **decreases** as we try action $a$ more\n",
    "- $\\log t$: Total time steps so far\n",
    "  - Grows slowly → Eventually we stop exploring even rarely-tried actions\n",
    "  - Ensures all actions get tried infinitely often (as $t \\to \\infty$)\n",
    "- $c$: Exploration parameter (typically 1 or 2)\n",
    "  - Higher $c$ → More exploration\n",
    "  - Lower $c$ → More exploitation\n",
    "\n",
    "### Why the Square Root and Logarithm?\n",
    "\n",
    "The formula $\\sqrt{\\log t / N_t(a)}$ comes from **Hoeffding's inequality** in probability theory. It provides a confidence bound on how far the true mean could be from our estimate, with high probability.\n",
    "\n",
    "Intuitively:\n",
    "- $1/N_t(a)$ is the variance of the sample mean\n",
    "- $\\sqrt{\\cdot}$ converts variance to standard deviation\n",
    "- $\\log t$ provides the confidence level that grows with time\n",
    "\n",
    "## Example Calculation\n",
    "\n",
    "Suppose at time $t = 6$ we have:\n",
    "\n",
    "**Action 1:**\n",
    "- $Q_6(1) = 2.0$\n",
    "- $N_6(1) = 3$ (selected 3 times)\n",
    "- $U_6(1) = \\sqrt{\\frac{\\log 6}{3}} = \\sqrt{\\frac{1.79}{3}} \\approx 0.77$\n",
    "- **UCB score** = $2.0 + 0.77 = 2.77$\n",
    "\n",
    "**Action 2:**\n",
    "- $Q_6(2) = 2.33$ (from rewards: 0, 3, 4)\n",
    "- $N_6(2) = 3$ (selected 3 times)\n",
    "- $U_6(2) = \\sqrt{\\frac{\\log 6}{3}} \\approx 0.77$\n",
    "- **UCB score** = $2.33 + 0.77 = 3.10$\n",
    "\n",
    "→ **Action 2 is selected** (higher UCB score)\n",
    "\n",
    "### Value Update Rule\n",
    "\n",
    "Like epsilon-greedy, UCB updates Q-values using sample averaging:\n",
    "\n",
    "$$Q_{t+1}(a) = Q_t(a) + \\frac{1}{N_t(a)}\\left[R_t - Q_t(a)\\right]$$\n",
    "\n",
    "The uncertainty bounds are updated automatically based on $N_t(a)$ and $t$.\n",
    "\n",
    "### Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- **Principled exploration**: Explores based on uncertainty, not randomly\n",
    "- **No hyperparameter tuning**: Works well with default $c=1$ or $c=2$\n",
    "- **Theoretical guarantees**: Logarithmic regret bound $O(\\log t)$\n",
    "- **Automatic balancing**: Naturally reduces exploration over time\n",
    "- **Never wastes time**: Always picks the most promising action\n",
    "\n",
    "**Limitations:**\n",
    "- **Assumes stationarity**: Works best when reward distributions don't change\n",
    "- **Slow for many actions**: Must try each action at least once initially\n",
    "- **No distributional model**: Doesn't model reward distributions (unlike Thompson Sampling)\n",
    "- **Sensitive to variance**: Assumes similar variance across actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deepmind_bandits import UpperConfidenceBound, GaussianBandits, BanditDataAnalyzer\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "We create a Gaussian bandit environment with 4 actions. Each action has a true mean reward (unknown to the agent) and some noise (standard deviation).\n",
    "\n",
    "**Environment Design:**\n",
    "- Action 0: Low mean (1.0), low noise - consistently mediocre\n",
    "- Action 1: High mean (2.0), moderate noise - **optimal action**\n",
    "- Action 2: Negative mean (-1.0), low noise - clearly bad\n",
    "- Action 3: Zero mean (0.0), high noise - unpredictable\n",
    "\n",
    "The agent must learn through interaction which action is best, using the UCB criterion to guide exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bandit environment with Gaussian reward distributions\n",
    "means = [1.0, 2.0, -1.0, 0.0]\n",
    "stds = [0.1, 0.2, 0.1, 0.3]\n",
    "env = GaussianBandits(means, stds)\n",
    "num_actions = env.num_arms\n",
    "\n",
    "print(f\"Number of actions: {num_actions}\")\n",
    "print(f\"True mean rewards: {means}\")\n",
    "print(f\"Reward noise (stds): {stds}\")\n",
    "print(f\"\\nOptimal action: {np.argmax(means)} with mean reward = {max(means):.2f}\")\n",
    "print(f\"\\nGoal: UCB will systematically explore uncertain actions and exploit the best!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Initialization\n",
    "\n",
    "We create a UCB agent and a data analyzer to track its performance over time.\n",
    "\n",
    "**Agent Configuration:**\n",
    "- All Q-values initialized to 0 (neutral starting point)\n",
    "- Exploration parameter $c = 1.0$ (standard choice)\n",
    "- Uncertainty bounds initialized to 0 (will be computed on first selection)\n",
    "- Uses sample-average updates (step size = 1/N)\n",
    "\n",
    "**How UCB will behave:**\n",
    "1. **Initially**: All actions have infinite uncertainty (N=0), so each is tried once\n",
    "2. **Early phase**: High uncertainty bonuses encourage broad exploration\n",
    "3. **Mid phase**: Uncertainty decreases, good actions get selected more\n",
    "4. **Late phase**: Optimal action dominates, but occasional exploration continues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UCB agent\n",
    "agent = UpperConfidenceBound(num_actions=num_actions, initial_q=0.0, c=1.0)\n",
    "\n",
    "# Create analyzer for tracking performance metrics\n",
    "analyzer = BanditDataAnalyzer(means, num_actions)\n",
    "\n",
    "print(f\"Agent Configuration:\")\n",
    "print(f\"  Exploration parameter c: {agent.c}\")\n",
    "print(f\"  Initial Q-values: {agent.Q}\")\n",
    "print(f\"  Initial uncertainty bounds: {agent.U}\")\n",
    "print(f\"  Initial action counts: {agent.N}\")\n",
    "print(f\"\\nUCB will automatically balance exploration and exploitation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now we run the main learning loop for 1000 time steps. At each step:\n",
    "\n",
    "1. **Compute UCB scores**: For each action $a$, calculate $Q_t(a) + c\\sqrt{\\log t / N_t(a)}$\n",
    "2. **Select**: Choose action with highest UCB score (deterministic, unlike epsilon-greedy)\n",
    "3. **Execute**: Take action in environment, observe reward\n",
    "4. **Update**: Update Q-value and uncertainty bound\n",
    "5. **Track**: Record performance metrics\n",
    "\n",
    "**What to expect:**\n",
    "- All actions tried at least once initially\n",
    "- Bad actions quickly ruled out (low Q + small uncertainty)\n",
    "- Promising actions get more chances (high Q or high uncertainty)\n",
    "- Optimal action eventually dominates\n",
    "- No wasted exploration on obviously bad actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000  # Number of time steps\n",
    "\n",
    "for t in range(T):\n",
    "    # Agent selects action using UCB criterion\n",
    "    action = agent.select_action()\n",
    "    \n",
    "    # Execute action and observe reward from environment\n",
    "    reward = env.pull_arm(action)\n",
    "    \n",
    "    # Update Q-value estimate and uncertainty bound\n",
    "    agent.update_values(action, reward)\n",
    "    \n",
    "    # Track performance for analysis\n",
    "    analyzer.update_and_analyze(action, reward)\n",
    "\n",
    "print(f\"Training completed: {T} time steps\\n\")\n",
    "print(f\"Final Q-value estimates:\")\n",
    "for a in range(num_actions):\n",
    "    error = abs(agent.Q[a] - means[a])\n",
    "    ucb_score = agent.Q[a] + agent.U[a]\n",
    "    print(f\"  Action {a}: Q = {agent.Q[a]:.3f}, U = {agent.U[a]:.3f}, UCB = {ucb_score:.3f}\")\n",
    "    print(f\"            (true = {means[a]:.2f}, error = {error:.3f})\")\n",
    "    \n",
    "print(f\"\\nAction selection counts: {agent.N}\")\n",
    "print(f\"Total selections: {sum(agent.N)} (should equal {T})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Let's analyze how well the UCB agent performed. We'll examine:\n",
    "- **Q-value convergence**: Did estimates converge to true values?\n",
    "- **Uncertainty evolution**: How did uncertainty bounds change over time?\n",
    "- **Action selection**: Did UCB efficiently identify the optimal action?\n",
    "- **Regret**: How does UCB compare to epsilon-greedy?\n",
    "- **Cumulative reward**: Total reward accumulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize analysis (compute cumulative metrics)\n",
    "analyzer.finalize_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Value Progression Over Time\n",
    "\n",
    "This plot shows how the agent's Q-value estimates evolved during learning:\n",
    "- **Solid colored lines**: Q-values for each action over time\n",
    "- **Dashed lines**: True mean rewards (ground truth)\n",
    "- **Red arrows**: When the agent switches between actions\n",
    "\n",
    "**What to look for:**\n",
    "- Q-values should converge toward true means (dashed lines)\n",
    "- UCB explores more systematically than epsilon-greedy\n",
    "- Bad actions (negative rewards) quickly abandoned\n",
    "- Fewer random switches compared to epsilon-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_Qvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Regret\n",
    "\n",
    "**Regret** measures the difference between optimal reward and actual reward.\n",
    "\n",
    "**Key difference from epsilon-greedy:**\n",
    "- Epsilon-greedy: Regret grows **linearly** (always explores randomly)\n",
    "- UCB: Regret grows **logarithmically** $O(\\log t)$ (exploration decreases over time)\n",
    "\n",
    "**What to look for:**\n",
    "- **Slope should flatten**: Unlike epsilon-greedy, UCB's regret growth slows down\n",
    "- **Lower final regret**: UCB should outperform epsilon-greedy\n",
    "- **Shape**: Should be sublinear (curve flattens over time)\n",
    "\n",
    "The logarithmic regret bound is UCB's main theoretical advantage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_regret()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Reward\n",
    "\n",
    "This shows the total reward accumulated over time:\n",
    "- **Black line**: Total cumulative reward across all actions\n",
    "- **Dashed lines**: Cumulative reward per action\n",
    "\n",
    "**What to look for:**\n",
    "- Steeper slope = higher reward rate\n",
    "- Optimal action (action 1) should dominate\n",
    "- Total reward should grow faster than epsilon-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_cumulative_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"UCB PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nExperiment Parameters:\")\n",
    "print(f\"  Total time steps: {T}\")\n",
    "print(f\"  Exploration parameter c: {agent.c}\")\n",
    "\n",
    "print(f\"\\nLearned Q-Values and UCB Scores:\")\n",
    "ucb_scores = agent.Q + agent.U\n",
    "for a in range(num_actions):\n",
    "    q_val = agent.Q[a]\n",
    "    u_val = agent.U[a]\n",
    "    ucb = ucb_scores[a]\n",
    "    true_mean = means[a]\n",
    "    error = abs(q_val - true_mean)\n",
    "    count = agent.N[a]\n",
    "    pct = 100 * count / T\n",
    "    marker = \" ← OPTIMAL\" if a == np.argmax(means) else \"\"\n",
    "    print(f\"  Action {a}: Q={q_val:6.3f}, U={u_val:5.3f}, UCB={ucb:6.3f}\")\n",
    "    print(f\"            True={true_mean:5.2f}, Error={error:.3f} | \"\n",
    "          f\"Selected {count:4d} times ({pct:5.1f}%){marker}\")\n",
    "\n",
    "optimal_action = np.argmax(means)\n",
    "optimal_selections = agent.N[optimal_action]\n",
    "optimal_pct = 100 * optimal_selections / T\n",
    "\n",
    "print(f\"\\nOptimal Action Performance:\")\n",
    "print(f\"  Optimal action: {optimal_action} (true mean = {means[optimal_action]:.2f})\")\n",
    "print(f\"  Times selected: {optimal_selections}/{T} ({optimal_pct:.1f}%)\")\n",
    "\n",
    "if optimal_pct >= 85:  # UCB should select optimal action >85% of the time\n",
    "    print(f\"  ✓ Excellent! UCB efficiently identified and exploited optimal action\")\n",
    "elif optimal_pct >= 70:\n",
    "    print(f\"  ✓ Good performance - learning the optimal action\")\n",
    "else:\n",
    "    print(f\"  ⚠ Could improve - may need more time to converge\")\n",
    "\n",
    "print(f\"\\nRegret Analysis:\")\n",
    "print(f\"  Final cumulative regret: {analyzer.regret[-1]:.2f}\")\n",
    "print(f\"  Average regret per step: {analyzer.regret[-1]/T:.3f}\")\n",
    "print(f\"  Expected regret growth: O(log t) - sublinear\")\n",
    "\n",
    "print(f\"\\nUncertainty Analysis:\")\n",
    "print(f\"  Final uncertainty bounds: {[f'{u:.3f}' for u in agent.U]}\")\n",
    "print(f\"  Interpretation: Lower uncertainty = more confident estimates\")\n",
    "print(f\"  Optimal action has lowest uncertainty: {agent.U[optimal_action] == min(agent.U)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Uncertainty Evolution\n",
    "\n",
    "Let's track how uncertainty bounds and UCB scores evolved over time. This visualization helps understand UCB's exploration strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a fresh experiment to track uncertainty evolution\n",
    "np.random.seed(42)\n",
    "agent_track = UpperConfidenceBound(num_actions=num_actions, c=1.0)\n",
    "uncertainty_history = [[] for _ in range(num_actions)]\n",
    "q_history = [[] for _ in range(num_actions)]\n",
    "\n",
    "T_track = 200\n",
    "for t in range(T_track):\n",
    "    action = agent_track.select_action()\n",
    "    reward = env.pull_arm(action)\n",
    "    agent_track.update_values(action, reward)\n",
    "    \n",
    "    # Track Q-values and uncertainty for all actions\n",
    "    for a in range(num_actions):\n",
    "        q_history[a].append(agent_track.Q[a])\n",
    "        uncertainty_history[a].append(agent_track.U[a])\n",
    "\n",
    "# Plot uncertainty bounds and UCB scores\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: Uncertainty bounds over time\n",
    "for a in range(num_actions):\n",
    "    ax1.plot(uncertainty_history[a], label=f'Action {a}', linewidth=2)\n",
    "ax1.set_xlabel('Time Steps', fontsize=12)\n",
    "ax1.set_ylabel('Uncertainty Bound U(a)', fontsize=12)\n",
    "ax1.set_title('Uncertainty Bounds Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.text(0.02, 0.98, 'Uncertainty decreases as\\nactions are selected more', \n",
    "         transform=ax1.transAxes, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Right plot: UCB scores over time\n",
    "for a in range(num_actions):\n",
    "    ucb_vals = np.array(q_history[a]) + np.array(uncertainty_history[a])\n",
    "    ax2.plot(ucb_vals, label=f'Action {a}', linewidth=2)\n",
    "    ax2.axhline(y=means[a], color=f'C{a}', linestyle='--', alpha=0.3, linewidth=1.5)\n",
    "ax2.set_xlabel('Time Steps', fontsize=12)\n",
    "ax2.set_ylabel('UCB Score (Q + U)', fontsize=12)\n",
    "ax2.set_title('UCB Scores Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.text(0.02, 0.98, 'Dashed lines = true means\\nUCB selects max score', \n",
    "         transform=ax2.transAxes, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"- Left plot: Uncertainty (exploration bonus) decreases as actions are tried\")\n",
    "print(\"- Right plot: UCB scores combine estimates + uncertainty\")\n",
    "print(\"- Bad actions quickly ruled out (low Q, low uncertainty)\")\n",
    "print(\"- Optimal action dominates once uncertainty is resolved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**What we learned:**\n",
    "1. ✅ UCB uses **optimism under uncertainty** for principled exploration\n",
    "2. ✅ Uncertainty bounds decrease with $1/\\sqrt{N_t(a)}$ - more samples = more confidence\n",
    "3. ✅ UCB achieves **logarithmic regret** $O(\\log t)$ - much better than linear!\n",
    "4. ✅ No hyperparameter tuning needed - works well with $c=1$ or $c=2$\n",
    "5. ✅ Automatically reduces exploration over time (unlike fixed epsilon)\n",
    "\n",
    "**Comparison with Epsilon-Greedy:**\n",
    "- **Exploration**: UCB explores systematically (uncertainty-based) vs. epsilon-greedy (random)\n",
    "- **Regret**: UCB has $O(\\log t)$ regret vs. epsilon-greedy has $O(t)$ regret  \n",
    "- **Efficiency**: UCB doesn't waste time on obviously bad actions\n",
    "- **Adaptability**: UCB automatically reduces exploration; epsilon-greedy needs manual decay\n",
    "\n",
    "**When to use UCB:**\n",
    "- ✅ When you want principled exploration without random noise\n",
    "- ✅ When regret minimization is important\n",
    "- ✅ In stationary environments (reward distributions don't change)\n",
    "- ✅ When you want theoretical guarantees\n",
    "- ✅ As a strong baseline before trying more complex methods\n",
    "\n",
    "**Limitations to consider:**\n",
    "- ⚠️ Assumes stationary rewards (use discounted UCB for non-stationary)\n",
    "- ⚠️ Sensitive to reward scale (normalize rewards if needed)\n",
    "- ⚠️ Doesn't model full reward distributions (Thompson Sampling does)\n",
    "\n",
    "**Next steps:**\n",
    "- Try different values of $c$ (exploration parameter)\n",
    "- Compare performance with epsilon-greedy on same problem\n",
    "- Explore Thompson Sampling for Bayesian approach\n",
    "- Study UCB variants: UCB1-Tuned, Discounted UCB, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
