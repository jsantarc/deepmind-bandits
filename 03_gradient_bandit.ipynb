{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Bandit Algorithm\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 matplotlib==3.8.4 torch==2.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Bandit Algorithms\n",
    "\n",
    "###  The Softmax Policy in Bandit Algorithms\n",
    "In the Gradient Bandit, the probability of choosing action $a$ (or policy) is given by the softmax function.\n",
    "\n",
    "$$\\pi_{\\theta}(a) = \\frac{e^{H_{\\theta}(a)}}{\\sum_{b=1}^{K} e^{H_{\\theta}(b)}}$$\n",
    "\n",
    "**$H_{\\theta}(a)$** is the learned numerical **preference** (or logit). This exponentiated values are positive; as such, their sum with respect to action $a$ is 1. The parameters $\\theta$ are obtained through training. We select the action with the largest probability.\n",
    "\n",
    "$$A_t = \\arg\\max_a\\{ \\pi_{\\theta}(a)\\} $$\n",
    "\n",
    "For example K=3: $\\pi_{\\theta}(1)=0.25$, $\\pi_{\\theta}(2)=0.50$ and $\\pi_{\\theta}(3)=0.25$\n",
    "\n",
    "$A_1 = \\arg\\max_a \\{0.25, 0.5,0.25 \\}$  \n",
    "$A_1 =2$\n",
    "\n",
    "### Action Preference ($H_{\\theta}(a)$) in the Model\n",
    "\n",
    "In the `SimpleLinearModel`, the action preference $H_{\\theta}(a)$ is the direct, linear output of the network *before* the Softmax is applied. This preference is calculated using the learned weights ($\\mathbf{w}$), the bias ($b$), and the fixed input vector ($\\mathbf{x}$):\n",
    "\n",
    "$$H_{\\theta}(a) = \\mathbf{w}_a^T \\mathbf{x}$$\n",
    "\n",
    "Since the model fixes the input $\\mathbf{x}$ for $a=j$ the single feature $x_j=1$, this simplifies the preference to:\n",
    "\n",
    "$H_{\\theta}(a) = w_a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from deepmind_bandits import GaussianBandits, BanditDataAnalyzer\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Model Implementation\n",
    "\n",
    "In PyTorch, we can easily implement the softmax function. The input dimension is typically 1, and the output dimension corresponds to the number of classes. Unlike the standard softmax function, where all inputs are treated as features with equal weighting, we implement it within the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self):\n",
    "        x = torch.tensor([1.0])\n",
    "        # returns: (batch, output_dim) logits\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Derivation\n",
    "\n",
    "We have a policy parameterized by $\\theta$ that defines a probability distribution $\\pi_\\theta(a)$ over actions. Each action $a$ has a true mean reward $q(a)$. Our goal is to choose $\\theta$ so that the policy selects actions that maximize the expected reward. The expected reward under the policy is\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}[R \\mid \\pi_\\theta] = \\sum_a \\pi_\\theta(a)\\, q(a).\n",
    "$$\n",
    "\n",
    "Since we are optimizing with respect to $\\theta$, and the reward function $q(a)$ does not depend on $\\theta$, the gradient of the objective is\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "= \\nabla_\\theta \\sum_a \\pi_\\theta(a)\\, q(a)\n",
    "= \\sum_a q(a)\\, \\nabla_\\theta \\pi_\\theta(a).\n",
    "$$\n",
    "\n",
    "Using the log-derivative trick:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\pi_\\theta(a)\n",
    "= \\pi_\\theta(a)\\, \\nabla_\\theta \\log \\pi_\\theta(a).\n",
    "$$\n",
    "\n",
    "Substituting this gives\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "= \\sum_a q(a)\\, \\pi_\\theta(a)\\, \\nabla_\\theta \\log \\pi_\\theta(a).\n",
    "$$\n",
    "\n",
    "Since actions are drawn according to $A \\sim \\pi_\\theta(a)$, the sum above is an expectation:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "\\approx\n",
    "\\frac{1}{N}\\sum_{t=1}^N q(A_t)\\,\\nabla_\\theta \\log \\pi_\\theta(A_t),\n",
    "\\qquad A_t \\sim \\pi_\\theta.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE Update Rule\n",
    "\n",
    "In the bandit setting, we do not know the true action-value function $q(a)$. Instead, after selecting an action $A_t$, we observe a reward sample $R_t$, which is an unbiased estimator of $q(A_t)$.\n",
    "\n",
    "Since $q(a)$ is unknown, we replace it with the sampled reward $R_t$, yielding the Monte Carlo policy gradient estimator:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "\\approx R_t\\, \\nabla_\\theta \\log \\pi_\\theta(A_t).\n",
    "$$\n",
    "\n",
    "Using this estimator, we update the policy parameters via gradient ascent:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1}\n",
    "= \\theta_t + \\alpha_t\\, R_t\\, \\nabla_\\theta \\log \\pi_\\theta(A_t).\n",
    "$$\n",
    "\n",
    "This gives the **REINFORCE** update rule in the bandit setting:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha\\, R\\, \\nabla_\\theta \\log \\pi_\\theta(A).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "The loss function in PyTorch (cross-entropy with logits) is given by:\n",
    "\n",
    "$$\n",
    "\\ell_t\n",
    "= - \\sum_{a=1}^{C} \\mathbf{1}[A_t = a]\\,\n",
    "\\ln\\left( \\frac{e^{w_a}}{\\sum_{i=1}^{C} e^{w_i}} \\right).\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "\\ell_t = - \\ln\\!\\bigl(\\pi_\\theta(A_t)\\bigr).\n",
    "$$\n",
    "\n",
    "To recover the bandit policy-gradient form, we multiply by the reward $R_t$:\n",
    "\n",
    "$$\n",
    "\\ell_t = -\\, R_t\\, \\ln\\!\\bigl(\\pi_\\theta(A_t)\\bigr).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandit_loss(logits, actions, rewards):\n",
    "    \"\"\"Policy gradient loss for bandits.\"\"\"\n",
    "    rewards = torch.tensor(rewards, requires_grad=False)\n",
    "    return rewards * F.cross_entropy(logits, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "means = [1.0, 2.0, -1.0, 0.0]\n",
    "stds = [0.1, 0.2, 0.1, 0.3]\n",
    "env = GaussianBandits(means, stds)\n",
    "num_arms = env.num_arms\n",
    "num_actions = num_arms\n",
    "\n",
    "print(f\"Number of arms: {num_arms}\")\n",
    "print(f\"True means: {means}\")\n",
    "print(f\"Optimal arm: {np.argmax(means)} (mean={max(means)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "lr = 0.1\n",
    "T = 1000\n",
    "\n",
    "model = SimpleLinearModel(1, num_actions)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Create analyzer\n",
    "analyzer = BanditDataAnalyzer(means, num_actions)\n",
    "\n",
    "# Baseline (running mean)\n",
    "running_mean = 0.0\n",
    "n = 1\n",
    "\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"Total steps: {T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_(z):\n",
    "    \"\"\"Get argmax and detach from computation graph.\"\"\"\n",
    "    return torch.argmax(z.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(T):\n",
    "    # Forward pass: get logits (action preferences)\n",
    "    logits = model()\n",
    "    action = torch.tensor(argmax_(logits))\n",
    "    \n",
    "    # Pull arm and observe reward\n",
    "    reward = env.pull_arm(action)\n",
    "    \n",
    "    # Update running mean (baseline)\n",
    "    running_mean = running_mean + (reward - running_mean) / n\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = bandit_loss(logits, action, running_mean)\n",
    "    \n",
    "    # Backprop and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track performance\n",
    "    analyzer.update_and_analyze(action.item(), reward, loss_sample=loss)\n",
    "\n",
    "print(f\"\\nTraining completed: {T} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.finalize_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Value Progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_Qvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Regret and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_regret()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_cumulative_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Weights (Learned Preferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Gradient Bandit Performance Summary ===\")\n",
    "print(f\"\\nLearned action preferences (weights):\")\n",
    "with torch.no_grad():\n",
    "    final_logits = model()\n",
    "    probs = F.softmax(final_logits, dim=0)\n",
    "    for a in range(num_actions):\n",
    "        print(f\"  Arm {a}: H(a)={final_logits[a].item():.3f}, Ï€(a)={probs[a].item():.3f} (true mean={means[a]:.3f})\")\n",
    "\n",
    "optimal_arm = np.argmax(means)\n",
    "print(f\"\\nOptimal arm: {optimal_arm}\")\n",
    "print(f\"Final cumulative regret: {analyzer.regret[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Loss Distribution Histogram\n\nLet's visualize the distribution of loss values during training. This helps us understand:\n- How loss values are distributed\n- Whether learning stabilized over time\n- The range of loss values encountered",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Extract loss values (convert from tensors to numpy)\nloss_values = [loss.item() if torch.is_tensor(loss) else loss for loss in analyzer.loss]\n\n# Create histogram\nplt.figure(figsize=(10, 6))\nplt.hist(loss_values, bins=50, edgecolor='black', alpha=0.7)\nplt.xlabel('Loss Value', fontsize=12)\nplt.ylabel('Frequency', fontsize=12)\nplt.title('Distribution of Loss Values During Training', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\n\n# Add statistics\nmean_loss = np.mean(loss_values)\nmedian_loss = np.median(loss_values)\nplt.axvline(mean_loss, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_loss:.3f}')\nplt.axvline(median_loss, color='green', linestyle='--', linewidth=2, label=f'Median: {median_loss:.3f}')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Loss Statistics:\")\nprint(f\"  Mean: {mean_loss:.4f}\")\nprint(f\"  Median: {median_loss:.4f}\")\nprint(f\"  Std Dev: {np.std(loss_values):.4f}\")\nprint(f\"  Min: {np.min(loss_values):.4f}\")\nprint(f\"  Max: {np.max(loss_values):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}